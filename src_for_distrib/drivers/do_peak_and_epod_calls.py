#!/usr/bin/python

# calculate peak calls for all ipod samples of interest
# we read all of the instances to look at from a table of name/config file pairs

import argparse
import shutil
import os
import sys
import toml
import subprocess
import glob
import re
import numpy as np
import tempfile
import multiprocessing
from recombinator.iid_bootstrap import iid_bootstrap
import scipy.stats
from matplotlib import pyplot as plt

import pathlib

this_path = pathlib.Path(__file__).parent.absolute()
utils_path = os.path.join(this_path, "../src/utils")
sys.path.insert(0, utils_path)

import hdf_utils
import anno_tools as anno
import peak_utils as pu


def call_peaks(in_fname, out_path, cutoff, samp_name):
    '''Utility function used to wrap creation of peak calling subprocess
    into a function, thus allowing simple addition to a multiprocessing
    pool.

    Args:
    -----
    in_fname : str
        Input file name
    out_path : str
        Path to which results should be written
    cutoff : float
        Value above which regions will be called peaks
    samp_name : str
        Name of this sample, i.e., IPOD, NC, etc.

    Returns:
    --------
    Path to the narrowpeak file generated by peak calling.
    '''
    base_name = os.path.basename(in_fname)

    base_name_prefix = os.path.splitext(base_name)[0]

    out_np_path = os.path.join(
        out_path,
        "{}_cutoff_{}_peaks.narrowpeak".format(
            base_name_prefix,
            cutoff,
        ),
    )

    run_cmd = PEAK_CALL_SCRIPT.format(
        in_fname,
        samp_name,
        out_np_path,
        cutoff,
    )
    subprocess.call(run_cmd, shell=True)

    return out_np_path

def call_epods(in_fname, compare_fname, out_path, invert=False):
    '''Utility function used to wrap creation of epod calling subprocess
    into a function, thus allowing simple addition to a multiprocessing
    pool.

    Args:
    -----
    in_fname : str
        Input file name
    compare_fname : str
        Name of file containing data against which the input data will
        be compared in EPOD calling.
    out_path : str
        Path to which results should be written

    Returns:
    --------
    Path to the narrowpeak file generated by epod calling.
    '''

    base_name = os.path.basename(compare_fname)
    base_name_prefix = os.path.splitext(base_name)[0]

    out_prefix = os.path.join(
        out_path,
        base_name_prefix,
    )

    if invert:
        out_prefix += "_inverted"

    loose_epod_outfile = "{}_epods_loose.narrowpeak".format(out_prefix)
    strict_epod_outfile = "{}_epods_strict.narrowpeak".format(out_prefix)

    epod_cmd = EPOD_CALL_SCRIPT.format(
        in_fname,
        compare_fname,
        out_prefix,
    )
    print("Using command \"{}\" to call EPODS.".format(epod_cmd))
    subprocess.call(epod_cmd, shell=True)

    return (loose_epod_outfile, strict_epod_outfile)

def generate_fname(samp, chipsub_samps, score_type, out_prefix, invert):

    # if the sample was in the chipsub category, its dset name
    #   looks something like this
    if samp in chipsub_samps:
        if score_type == 'rz':
            base_fname = "{}_{}_rzchipsub".format(
                out_prefix, samp.upper()
            )
        elif score_type == 'log10p':
            base_fname = "{}_{}_rzchipsublog10p".format(
                out_prefix, samp.upper()
            )

    # if the sample did NOT have chipsub performed, its dset name
    #   looks something like this.
    else:
        if score_type == 'rz':
            base_fname = "{}_{}_vs_inp_rzlograt".format(
                out_prefix, samp.upper()
            )
        elif score_type == 'log10p':
            base_fname = "{}_{}_vs_inp_rzlogratlog10p".format(
                out_prefix, samp.upper()
            )

    # we only do inverted calls on non-chipsubbed stuff
    if invert:
        if score_type == 'rz':
            base_fname = "{}_{}_vs_inp_rzlograt".format(
                out_prefix, samp.upper()
            )
        elif score_type == 'log10p':
            base_fname = "{}_{}_vs_inp_rzlogratlog10p".format(
                out_prefix, samp.upper()
            )

    fname = base_fname + "_{}.bedgraph"

    return fname,base_fname


def calc_idr(paired, out_files, ctg_lut, out_path,
             fname, mean_fname, in_path, idr_thresh, invert=False,
             signal_type="peak", epod_type=None, cutoff=None):

    if paired:
        # go over replicates' peaks and do pair-wise IDR calculation
        #   for each pair-wise grouping of replicates
        # Save narrowpeak output for each IDR calculation
        rep_count = len(out_files)
        n_idrs = (rep_count**2 - rep_count) / 2
        for ctg_idx,ctg_info in ctg_lut.items():
            ctg_len = ctg_info["length"]
            ctg_array_dict[ctg_info["id"]]["num_passed_array"] = np.zeros(
                int(ctg_len/RESOLUTION)
            )

        rep_idxs = np.asarray([i for i in range(rep_count)])
        idr_outfiles = []
        placeholder_files = []
        
        for idx_a in rep_idxs:
            for idx_b in rep_idxs[rep_idxs > idx_a]:

                fname_a = out_files[idx_a]
                pref_a = os.path.splitext(
                    os.path.basename(fname_a)
                )[0]
                fname_b = out_files[idx_b]
                pref_b = os.path.splitext(
                    os.path.basename(fname_b)
                )[0]
                idr_out_pref = "{}_vs_{}_idr".format(
                    pref_a,
                    pref_b,
                )
                if invert:
                    idr_out_pref += "_inverted"
                idr_out_pref = os.path.join(
                    out_path,
                    idr_out_pref
                )
                idr_outfile = idr_out_pref + ".narrowpeak"
                idr_outfiles.append(idr_outfile)

                # create empty narrowpeak files if the files don't
                #  already exits.
                if not os.path.isfile(fname_a):
                    pathlib.Path(fname_a).touch()
                    placeholder_files.append(fname_a)

                if not os.path.isfile(fname_b):
                    pathlib.Path(fname_b).touch()
                    placeholder_files.append(fname_b)
                
                print("Calculating IDR for each {} in {} and {}.".format(signal_type, fname_a, fname_b))
                idr_cmd = PEAK_IDR_SCRIPT.format(
                    fname_a,
                    fname_b,
                    idr_out_pref,
                    idr_outfile,
                )
                subprocess.call(idr_cmd, shell=True)
                # check whether output file was written. If not,
                #   set up empty narrowpeak file
                if not os.path.isfile(idr_outfile):
                    pathlib.Path(idr_outfile).touch()
                    placeholder_files.append(idr_outfile)

        if signal_type == "epod":
            if epod_type is None:
                sys.exit("ERROR: you specified signal_type = \"epod\", but did not provide an epod_type as either \"loose\" or \"strict\". Re-run, specifying loose or strict.")

        base_name = os.path.basename(fname)
        final_fname = pu.compile_idr_results(
            idr_outfiles,
            ctg_array_dict,
            RESOLUTION,
            base_name,
            mean_fname,
            in_path,
            out_path,
            idr_thresh,
            invert = invert,
            signal_type = signal_type,
            epod_type = epod_type,
            cutoff = cutoff,
        )

        return final_fname,placeholder_files

def get_kl_divergences(peak_scores, nonpeak_scores,
                    lowest_bin=-10, highest_bin=11, alpha=0.05, n_b=1000):
    """Calculate KL divergence between peaks and non-peaks. Also
    performs bootstrapping to get the upper and lower confidence limits
    for the KL divergence.
    """

    if peak_scores.size == 0:
        return (0,0,0)
    elif peak_scores.size == 1:
        peak_scores = np.expand_dims(peak_scores, -1)

    bins = np.arange(lowest_bin, highest_bin)
    counts_1,bins_1 = np.histogram(peak_scores, bins=bins)
    counts_2,bins_2 = np.histogram(nonpeak_scores, bins=bins)

    entropy=scipy.stats.entropy( counts_1+1, counts_2+1 )

    value_distr_1 = iid_bootstrap(peak_scores, replications=n_b, replace=True)
    value_distr_2 = iid_bootstrap(nonpeak_scores, replications=n_b, replace=True)

    entropy_from_bootstrap=[]

    for i in range(n_b):
        counts_1, bins_1 = np.histogram(
            value_distr_1[i,:],
            bins=bins,
        )
        counts_2, bins_2 = np.histogram(
            value_distr_2[i,:],
            bins=bins,
        )
        entropy_boot = scipy.stats.entropy( counts_1+1, counts_2+1 )
        entropy_from_bootstrap.append( entropy_boot )

    # now get the confidence interval based on percentiles
    cl_lo = scipy.stats.scoreatpercentile( entropy_from_bootstrap, 100*(alpha/2) )
    cl_hi = scipy.stats.scoreatpercentile( entropy_from_bootstrap, 100*(1- (alpha/2)) )

    return (cl_lo, entropy, cl_hi)

           
def choose_final_threshold(idr_files, ctg_lut, spike_name, mean_fname,
                        lowest_bin=-10, highest_bin=11, alpha=0.05, n_b=1000):
    """Get the cutoff index that we consider to be the best cutoff for
    robustly distinguishing between peak and non-peak regions of the genome.
    """

    divergences = []
    print(idr_files)
    for i,idr_fname in enumerate(idr_files):
        print("=============================")
        print("Calculating KL divergence between peaks in {} and non-peaks.".format(idr_fname))
        print("=============================")

        final_peaks = anno.NarrowPeakData()
        final_peaks.parse_narrowpeak_file(idr_fname)
        complement_bed_data = final_peaks.fetch_complement_bed_data(
            contig_lut = ctg_lut,
            filter_chrs = [spike_name],
        )

        tmp_dir = tempfile.TemporaryDirectory()

        # write the bed file containing peak-less regions to bed file
        complement_bed_data.fname = '/home/schroedj/nopeak_data.bed'
        complement_bed_data.write_file()

        nonpeak_bed_fname = os.path.join(tmp_dir.name, 'nopeak_data.bed')
        complement_bed_data.fname = nonpeak_bed_fname
        complement_bed_data.write_file()

        # write mean peak and non-peak scores to temporary files
        peak_score_fname = os.path.join(tmp_dir.name, 'peakscore.bed')
        #peak_score_fname = '/home/schroedj/peakscore.bed'
        nonpeak_score_fname = os.path.join(tmp_dir.name, 'nonpeakscore.bed')
        #nonpeak_score_fname = '/home/schroedj/nonpeakscore.bed'

        #if not os.path.isfile(final_peaks.fname):
        #    raise()
        #if not os.path.isfile(mean_fname):
        #    raise()
        #if not os.path.isfile(nopeak_bed_fname):
        #    raise()
        #if not os.path.isdir(tmp_dir.name):
        #    raise()
        bed_map_cmd = "bedtools map \
                -a {} \
                -b {} \
                -o mean -c 4 \
                | cut -f 7 \
                > {}".format(
                final_peaks.fname,
                mean_fname,
                peak_score_fname,
        )
        print(bed_map_cmd)
        subprocess.call(
            bed_map_cmd,
            shell = True,
        )
        peak_scores = np.loadtxt(peak_score_fname)
        
        bed_map_cmd = "bedtools map \
                -a {} \
                -b {} \
                -o mean -c 4 \
                | cut -f 7 \
                > {}".format(
                nonpeak_bed_fname,
                mean_fname,
                nonpeak_score_fname,
        )
        print(bed_map_cmd)
        subprocess.call(
            bed_map_cmd,
            shell = True,
        )
        nonpeak_scores = np.loadtxt(nonpeak_score_fname)

        tmp_dir.cleanup()
        print(nonpeak_scores)

        # give a tuple of (lower_cl, observed, upper_cl) for the KL divergence
        this_div_info = get_kl_divergences(
            peak_scores,
            nonpeak_scores,
            lowest_bin,
            highest_bin,
            alpha,
            n_b,
        )
        divergences.append(this_div_info)
    # make divergences an array, and transpose it and slice rows in reverse
    #  so final array's rows are upper, observed, lower at indices 0,1,2, repectively,
    #  and columns are cutoffs
    div_arr = np.asarray(divergences).T[::-1,:]
    max_observed = div_arr[1,:].max()
    # get max index at which upper cl is greater than the max observed KL divergence
    cutoff_idx = np.where(div_arr[0,:] > max_observed)[0].max()
    return (cutoff_idx, div_arr, max_observed)
    
 
def process_sample(line, conf_dict_global, invert):

    dirname,samp_conf = line.rstrip().split()
    dir_path = os.path.join(BASEDIR, dirname)
    os.chdir(dir_path)

    conf_dict = toml.load(os.path.join(dir_path, samp_conf))
    out_file_prefix = conf_dict["general"]["out_prefix"]
    chipsub_samps = conf_dict["quant"]["chipsub_numerators"]
    no_chipsub_samps = conf_dict["quant"]["no_chipsub"]
    spike_name = conf_dict_global["genome"]["spike_in_name"]

    in_path = os.path.join(dir_path, conf_dict_global["bootstrap"]["output_path"])
    peak_out_path = os.path.join(dir_path, conf_dict_global["peaks"]["output_path"])
    best_thresh_path = os.path.join(peak_out_path, 'best_threshold')
    epod_out_path = os.path.join(dir_path, conf_dict_global["epods"]["output_path"])
    alpha = conf_dict_global["quant"]["alpha"]
    if isinstance(alpha, float):
        alpha = [alpha]

    if not os.path.isdir(peak_out_path):
        os.mkdir(peak_out_path)
    if not os.path.isdir(best_thresh_path):
        os.mkdir(best_thresh_path)
    if not os.path.isdir(epod_out_path):
        os.mkdir(epod_out_path)

    paired = conf_dict["quant"]["paired"]

    all_samps = []
    all_samps.extend(chipsub_samps)
    all_samps.extend(no_chipsub_samps)
    # manually place "chip" here to call peaks in the RNAP chip data
    no_chipsub_samps.append("chip")
    all_samps.append("chip")

    cutoff_dict = {
        'rz': conf_dict_global["peaks"]["rz_thresholds"],
        'log10p': conf_dict_global["peaks"]["log10p_thresholds"],
    }

    idr_threshold = conf_dict_global["idr"]["threshold"]

    for score_type in ['rz','log10p']:
        these_cutoffs = cutoff_dict[score_type]

        # loop over all samples
        for samp in all_samps:

            # fname_for_sub still has a {} at the end for formatting later
            fname_for_sub,base_fname = generate_fname(
                samp,
                chipsub_samps,
                score_type,
                out_file_prefix,
                invert,
            )

            fname = os.path.join(in_path, fname_for_sub)

            # If these data were not from paired samples of inp/chip/ipod,
            #   then just use the mean result for peak calling
            if not paired:
                fname_list = [ fname.format("mean") ]
                mean_fname = fname_list[0]
            # If the data were from paired samples of inp/chip/ipod,
            #   then get each replicate's dataset name.
            else:
                fname_search = fname.format("rep*")
                fname_list = glob.glob(os.path.join(in_path, fname_search))
                mean_fname = fname.format("mean")

            # do peak calling
            if not 'peaks' in skipsteps:

                # we'll collect the IDR-passing peaks' narrowpeak files in this list
                idr_files = []
                # loop over multiple score cutoffs.
                for cutoff in these_cutoffs:

                    # loop over files. Just one if it's not paired data.
                    out_files = []
                    for peak_fname in fname_list:

                        out_files.append(
                            call_peaks(
                                peak_fname,
                                peak_out_path,
                                cutoff,
                                samp,
                            )
                        )

                    if paired:
                        if len(fname_list) < 2:
                            print("============================")
                            print("Only one replicate found in sample {}, skipping IDR calculation for peaks.".format(samp))
                            print("----------------------------")
                            continue

    
                    idr_fname,placeholder_files = calc_idr(
                        paired,
                        out_files,
                        ctg_lut,
                        peak_out_path,
                        fname,
                        mean_fname,
                        in_path,
                        idr_threshold,
                        signal_type = "peak",
                        cutoff = cutoff,
                    )
                    idr_files.append(idr_fname)

                    # create empty narrowpeak files if the files don't
                    #  already exits.
                    if not os.path.isfile(idr_fname):
                        pathlib.Path(idr_fname).touch()
                        placeholder_files.append(idr_fname)

                if score_type == 'log10p':
                    lowest = 0
                    highest = 20
                elif score_type == 'rz':
                    lowest = -10
                    highest = 11

                best_cutoff_idx,kl_divs,max_obs_kl_div = choose_final_threshold(
                    idr_files,
                    ctg_lut,
                    spike_name,
                    mean_fname,
                    lowest,
                    highest,
                )
                best_cutoff = these_cutoffs[best_cutoff_idx]
                best_result = idr_files[best_cutoff_idx]
                basename = os.path.basename(best_result)
                print(best_result)
                print(basename)
                print(best_thresh_path)
                shutil.copy(best_result, os.path.join(best_thresh_path, basename))

                plt_name = os.path.join(best_thresh_path, "kl_div_cutoff_{}.png".format(basename))

                err_bar_arr = kl_divs[[2,0],:] - kl_divs[1,:][None,:]
                err_bar_arr[0,:] *= -1

                fig, ax = plt.subplots(figsize=(9,5))
                ax.axhline(
                    y=kl_divs[1,:].max(),
                    color='r',
                    linestyle='--',
                )
                ax.errorbar(
                    x = these_cutoffs,
                    y = kl_divs[1,:],
                    yerr = err_bar_arr,
                    color='tab:blue',
                    ecolor='tab:blue',
                )
                ax.set_xlabel('threshold')
                ax.set_ylabel('KL divergence')
                ax.spines['top'].set_visible(False)
                ax.spines['right'].set_visible(False)

                plt.savefig(plt_name)
                plt.close()

                # if we made any empty narrowpeak files, delete them now.
                if len(placeholder_files) > 0:
                    for fname in placeholder_files:
                        print("Removing empty placeholder file {}.".format(fname))
                        os.remove(fname)

            # do epod calling
            if not 'epods' in skipsteps:

                if samp == "chip":
                    continue

                if score_type == 'log10p':
                    continue

                # get mean fname to call epods in each
                mean_fname = fname.format("mean")

                if paired:

                    # get replicate fnames
                    fname_search = fname.format("rep*")
                    rep_fname_list = glob.glob(os.path.join(in_path, fname_search))

                    strict_epod_outfiles = []
                    loose_epod_outfiles = []

                    for epod_fname in rep_fname_list:

                        these_outfiles = call_epods(
                            mean_fname,
                            epod_fname,
                            epod_out_path,
                            invert,
                        )
                        loose_epod_outfiles.append(these_outfiles[0])
                        strict_epod_outfiles.append(these_outfiles[1])

                    base_name = os.path.basename(epod_fname)
                    base_name_prefix = os.path.splitext(base_name)[0]
                    out_np_path = os.path.join(
                        epod_out_path,
                        base_fname + "_{}_merged_epods.narrowpeak",
                    )

                    if invert:
                        loose_outfile_str = out_np_path.format("inverted_loose")
                        strict_outfile_str = out_np_path.format("inverted_strict")
                    else:
                        loose_outfile_str = out_np_path.format("loose")
                        strict_outfile_str = out_np_path.format("strict")

                    print("Writing merged loose epods to {}".format(loose_outfile_str))
                    print("Writing merged strict epods to {}".format(strict_outfile_str))

                    loose_infile_str = ' '.join(loose_epod_outfiles)
                    subprocess.call(
                        MERGE_SCRIPT.format(
                            loose_infile_str,
                            loose_outfile_str,
                        ),
                        shell=True,
                    )
                    
                    strict_infile_str = ' '.join(strict_epod_outfiles)
                    subprocess.call(
                        MERGE_SCRIPT.format(
                            strict_infile_str,
                            strict_outfile_str,
                        ),
                        shell=True,
                    )

                _ = call_epods(
                    mean_fname,
                    mean_fname,
                    epod_out_path,
                    invert,
                )

    return fname


if __name__ == "__main__":
    # parse command line arguments
    parser = argparse.ArgumentParser()

    parser.add_argument(
        'main_conf',
        help="Configuration file defining work to be done.",
    )
    parser.add_argument(
        '--skipsteps',
        help="comma-separated list of steps to skip. Can be any of (peaks,epods)."
    )
    parser.add_argument(
        '--invert_scores',
        help="Setting this option will call extended regions of depleted protein occupancy",
        action="store_true",
    )
    parser.add_argument(
        '--debug',
        action='store_true',
        help="Set this option to enable debugging",
    )
    args = parser.parse_args()

    if args.skipsteps is None:
        skipsteps = set()
    else:
        skipsteps = set(args.skipsteps.split(','))

    steps = ['peaks', 'epods']
    for step in skipsteps:
        if step not in steps:
            sys.exit("\nERROR: {} is not a step. Allowed steps are peaks and epods.\n".format(step))

    # parse the top level config file to get some needed information
    conf_file = args.main_conf
    conf_dict_global = toml.load(conf_file)

    BASEDIR = conf_dict_global["general"]["basedir"]
    BINDIR = conf_dict_global["general"]["bindir"]
    RESOLUTION = conf_dict_global["genome"]["resolution"]
    WINSIZE = conf_dict_global["peaks"]["windowsize_bp"] // RESOLUTION
    SAMP_FNAME = os.path.join(
        BASEDIR,
        conf_dict_global["general"]["condition_list"],
    )
    SEQ_DB = conf_dict_global["genome"]["genome_base"]
    PEAK_PROCS = conf_dict_global["peaks"]["nproc"]
    EPOD_PROCS = conf_dict_global["epods"]["nproc"]

    #TODO: add some info bout this stuff
    PEAK_CALL_SCRIPT = "python {}/peakcalling/call_peaks.py\
                            --in_file {{}}\
                            --sample_type {{}}\
                            --out_file {{}}\
                            --window_size {}\
                            --threshold {{}}".format(BINDIR,WINSIZE)

    PEAK_IDR_SCRIPT = "idr --samples {} {}\
                           --peak-merge-method avg \
                           --allow-negative-scores\
                           --use-nonoverlapping-peaks\
                           --plot --log-output-file {}.log --verbose\
                           --output-file {}"
    
    ## this one just need the peaks .gr file and output prefix
    #OVERLAP_SCRIPT = "python {}/peakcalling/analyze_peaks.py {{}}\
    #                  /data/petefred/st_lab_work/e_coli_data/regulondb_20180516/BindingSites_knownsites_flags.gr > \
    #                 {{}}_tf_overlaps.txt".format(
    #    BINDIR,
    #)

    EPOD_CALL_SCRIPT = "python {}/epodcalling/call_epods.py\
                            --main_conf {}\
                            --in_file {{}}\
                            --compare_file {{}}\
                            --out_prefix {{}}\
                            --resolution {}".format(
        BINDIR,
        os.path.abspath(args.main_conf),
        RESOLUTION,
    )

    INVERT = args.invert_scores

    if INVERT:
        EPOD_CALL_SCRIPT += " --invert_scores"

    MERGE_SCRIPT = "python {}/epodcalling/merge_epods.py\
                        --infiles {{}}\
                        --outfile {{}}".format(BINDIR)

    # read in toml file containing info on singularity versions if we're running this
    #   from within a singularity container
    if "IPOD_VER" in os.environ:
        VERSION = os.environ["IPOD_VER"]
        ver_filepath = os.path.join(BASEDIR, "singularity_version_info.toml")
        if os.path.isfile(ver_filepath):
            ver_info = toml.load(ver_filepath)
        else:
            ver_info = {
                "preprocessing": None,
                "alignment": None,
                "bootstrapping": None,
                "qc": None,
                "qnorm": None,
                "spikenorm": None,
                "quant": None,
                "peak_calls": None,
                "epod_calls": None,
            }

    ## get contig lengths using hdf_utils.make_ctg_lut_from_bowtie
    ## then make arrays for each contig to store peak loci passing
    ## IDR threshold
    ctg_lut = hdf_utils.make_ctg_lut_from_bowtie(SEQ_DB)
    ctg_array_dict = {}
    for ctg_idx,ctg_info in ctg_lut.items():
        ctg_len = ctg_info["length"]
        # now we have a dictionary with ctg id as keys, zeros array as vals
        ctg_array_dict[ctg_info["id"]] = {}
        ctg_array_dict[ctg_info["id"]]["loci"] = np.arange(0, ctg_len, RESOLUTION)

    # now go through the conditions of interest and run the analysis
    # we actually call the peaks, and then compare them to tfbs lists

    # use multiprocessing to do all of this in parallel
    if not args.debug:
        pool = multiprocessing.Pool(EPOD_PROCS)
        all_res = []

        samp_file = open(SAMP_FNAME)
        for line in samp_file:

            all_res.append(
                pool.apply_async(
                    process_sample,
                    [
                        line,
                        conf_dict_global,
                        INVERT,
                    ]
                )
            )

        pool.close()
        pool.join()

        n_err = 0
        for res in all_res:
            if not res.successful():
                print("\n==============================")
                print("Encountered error processing {}.".format(res.get()))
                print("------------------------------\n")
                n_err += 1
        print("Finished running peak and epod calling jobs. Encountered {} errors.".format(n_err))

    else:
        samp_file = open(SAMP_FNAME)
        for line in samp_file:
            process_sample(
                line,
                conf_dict_global,
                INVERT,
            )
        print("Finished running peak and epod calling jobs.")


    if "IPOD_VER" in os.environ:
        if n_err == 0:
            ver_info["peak_calls"] = VERSION
            ver_info["epod_calls"] = VERSION
            with open(ver_filepath, "w") as f:
                toml.dump(ver_info, f)

        #        analyze_cmd = OVERLAP_SCRIPT.format(
        #            os.path.join(
        #                args.outdir,
        #                dirname + "_rz_cutoff_{}_peaks.gr".format(cutoff),
        #            ),
        #            os.path.join(
        #                args.outdir,
        #                dirname + "_rz_cutoff_{}_peaks.gr".format(cutoff),
        #            ),
        #        )
        #        subprocess.call(analyze_cmd, shell=True)
        #
        #    gr_file = os.path.join(
        #        args.basedir,
        #        dirname,
        #        conf_dict["general"]["output_path"],
        #        conf_dict["general"]["out_prefix"] + "_v6rzlog10p_chipsub.gr",
        #    )
        #
        #    for cutoff in [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 10.0, 12.0, 15.0, 20.0, 30.0, 50.0]:
        #
        #        run_cmd = PEAK_CALL_SCRIPT.format(
        #            gr_file,
        #            os.path.join(
        #                args.outdir,
        #                dirname + "_log10p_cutoff_{}_peaks.gr".format(cutoff),
        #            ),
        #            cutoff,
        #        )
        #        subprocess.call(run_cmd, shell=True)
        #
        #        analyze_cmd = OVERLAP_SCRIPT.format(
        #            os.path.join(
        #                args.outdir,
        #                dirname + "_log10p_cutoff_{}_peaks.gr".format(cutoff),
        #            ),
        #            #NOTE: I think we want to get rid of the .gr suffix below.
        #            os.path.join(
        #                args.outdir,
        #                dirname + "_log10p_cutoff_{}_peaks.gr".format(cutoff),
        #            ),
        #        )
        #        subprocess.call(analyze_cmd, shell=True)

